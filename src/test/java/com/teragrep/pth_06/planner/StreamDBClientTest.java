/*
 * Teragrep Archive Datasource (pth_06)
 * Copyright (C) 2021-2024 Suomen Kanuuna Oy
 *
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU Affero General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU Affero General Public License for more details.
 *
 * You should have received a copy of the GNU Affero General Public License
 * along with this program.  If not, see <https://www.gnu.org/licenses/>.
 *
 *
 * Additional permission under GNU Affero General Public License version 3
 * section 7
 *
 * If you modify this Program, or any covered work, by linking or combining it
 * with other code, such other code is not for that reason alone subject to any
 * of the requirements of the GNU Affero GPL version 3 as long as this Program
 * is the same Program as licensed from Suomen Kanuuna Oy without any additional
 * modifications.
 *
 * Supplemented terms under GNU Affero General Public License version 3
 * section 7
 *
 * Origin of the software must be attributed to Suomen Kanuuna Oy. Any modified
 * versions must be marked as "Modified version of" The Program.
 *
 * Names of the licensors and authors may not be used for publicity purposes.
 *
 * No rights are granted for use of trade names, trademarks, or service marks
 * which are in The Program if any.
 *
 * Licensee must indemnify licensors and authors for any liability that these
 * contractual assumptions impose on licensors and authors.
 *
 * To the extent this program is licensed as part of the Commercial versions of
 * Teragrep, the applicable Commercial License may apply to this file if you as
 * a licensee so wish it.
 */
package com.teragrep.pth_06.planner;

import com.teragrep.pth_06.config.Config;
import com.teragrep.pth_06.jooq.generated.journaldb.tables.records.CorruptedArchiveRecord;
import com.teragrep.pth_06.jooq.generated.journaldb.tables.records.LogfileRecord;
import nl.jqno.equalsverifier.EqualsVerifier;
import org.jooq.*;
import org.jooq.impl.DSL;
import org.jooq.types.ULong;
import org.jooq.types.UShort;
import org.junit.jupiter.api.AfterEach;
import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.Disabled;
import org.testcontainers.containers.MariaDBContainer;
import org.testcontainers.utility.DockerImageName;
import org.testcontainers.utility.MountableFile;

import java.sql.Connection;
import java.sql.Date;
import java.sql.DriverManager;
import java.sql.Timestamp;
import java.time.Instant;
import java.time.ZoneId;
import java.time.ZonedDateTime;
import java.util.HashMap;
import java.util.Map;
import java.util.concurrent.ThreadLocalRandom;

import static com.teragrep.pth_06.jooq.generated.journaldb.Journaldb.JOURNALDB;

class StreamDBClientTest {

    private MariaDBContainer<?> mariadb;
    private Connection connection;
    private ZoneId zoneId = ZoneId.of("America/New_York");
    private final String streamDBUsername = "streamdb";
    private final String streamDBPassword = "streamdb_pass";

    private final String streamdbName = "streamdb";
    private final String journaldbName = "journaldb";
    private final Map<String, String> opts = new HashMap<String, String>() {

        {
            put("S3endPoint", "mock");
            put("S3identity", "mock");
            put("S3credential", "mock");
            put("DBusername", streamDBUsername);
            put("DBpassword", streamDBPassword);
            put("DBstreamdbname", streamdbName);
            put("DBjournaldbname", journaldbName);
            put("queryXML", "<index value=\"example\" operation=\"EQUALS\"/>");
            put("archive.enabled", "true");
        }
    };

    @BeforeEach
    public void setup() {
        // Start mariadb testcontainer with timezone set to America/New_York (UTC-4). Also creates a second streamdb database inside the container alongside the default journaldb.
        mariadb = Assertions
                .assertDoesNotThrow(() -> new MariaDBContainer<>(DockerImageName.parse("mariadb:10.5")).withPrivilegedMode(false).withUsername(streamDBUsername).withPassword(streamDBPassword).withCommand("--character-set-server=utf8mb4", "--collation-server=utf8mb4_unicode_ci", "--default-time-zone=" + zoneId.getId()).withDatabaseName(journaldbName).withCopyFileToContainer(MountableFile.forClasspathResource("CREATE_STREAMDB_DB.sql"), "/docker-entrypoint-initdb.d/"));
        mariadb.start();
        connection = Assertions
                .assertDoesNotThrow(
                        () -> DriverManager
                                .getConnection(mariadb.getJdbcUrl(), mariadb.getUsername(), mariadb.getPassword())
                );
        // streamdb and journaldb is populated with test data during MariaDBContainer startup using CREATE_STREAMDB_DB.sql. Logfile table of journaldb is left empty for tests to populate it.
    }

    @AfterEach
    public void cleanup() {
        Assertions.assertDoesNotThrow(() -> connection.close());
        mariadb.stop();
    }

    private LogfileRecord logfileRecordForEpoch(long epoch, boolean hasNullEpochColumns) {
        Instant instant = Instant.ofEpochSecond(epoch);
        ZonedDateTime zonedDateTime = instant.atZone(zoneId); // expects path dates to be in same timezone as mariadb system timezone
        int year = zonedDateTime.getYear();
        // format 0 in front of string if 1-9
        String month = String.format("%02d", zonedDateTime.getMonthValue());
        String day = String.format("%02d", zonedDateTime.getDayOfMonth());
        String hour = String.format("%02d", zonedDateTime.getHour());

        String filename = "example.log-@" + epoch + "-" + year + month + day + hour + ".log.gz";
        String path = year + "/" + month + "-" + day + "/example.tg.dev.test/example/" + filename;
        LogfileRecord logfileRecord = new LogfileRecord(
                ULong.valueOf(ThreadLocalRandom.current().nextLong(0L, Long.MAX_VALUE)),
                Date.valueOf(zonedDateTime.toLocalDate()),
                Date.valueOf(zonedDateTime.plusYears(1).toLocalDate()),
                UShort.valueOf(1),
                path,
                null,
                UShort.valueOf(1),
                filename,
                new Timestamp(epoch),
                ULong.valueOf(120L),
                "sha256 checksum 1",
                "archive tag 1",
                "oldExample",
                UShort.valueOf(2),
                UShort.valueOf(1),
                ULong.valueOf(390L),
                ULong.valueOf(epoch),
                ULong.valueOf(epoch + (365 * 24 * 3600)),
                ULong.valueOf(epoch),
                ULong.valueOf(1),
                null
        );

        LogfileRecord nullEpochRecord = new LogfileRecord(
                ULong.valueOf(ThreadLocalRandom.current().nextLong(0L, Long.MAX_VALUE)),
                Date.valueOf(zonedDateTime.toLocalDate()),
                Date.valueOf(zonedDateTime.plusYears(1).toLocalDate()),
                UShort.valueOf(1),
                path,
                null,
                UShort.valueOf(1),
                filename,
                new Timestamp(epoch),
                ULong.valueOf(120L),
                "sha256 checksum 1",
                "archive tag 1",
                "oldExample",
                UShort.valueOf(2),
                UShort.valueOf(1),
                ULong.valueOf(390L),
                null,
                null,
                null,
                ULong.valueOf(1),
                null
        );

        if (hasNullEpochColumns) {
            return nullEpochRecord;
        }
        return logfileRecord;
    }

    /**
     * Testing that pullToSliceTable() pulls only a specific row from database according to the input parameter.
     */
    @Test
    public void pullToSliceTableSingleTest() {
        // Add test data to logfile table in journaldb.
        final DSLContext ctx = DSL.using(connection, SQLDialect.MYSQL);
        // Set logdate and logtime to 2023-10-04:22 UTC-4 and set epoch_hour in path to 2023-10-05:02 UTC.
        LogfileRecord logfileRecord = logfileRecordForEpoch(1696471200L, false);
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord).execute();
        // Set logdate and logtime to 2023-10-05:22 UTC-4 and set epoch_hour in path to 2023-10-06:02 UTC.
        LogfileRecord logfileRecord2 = logfileRecordForEpoch(1696471200L + 24L * 3600L, false);
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord2).execute();

        // Assert StreamDBClient methods work as expected with the test data.
        final Map<String, String> opts = new HashMap<>(this.opts);
        opts.put("DBurl", mariadb.getJdbcUrl());
        final Config config = new Config(opts);
        Assertions.assertDoesNotThrow(() -> {
            try (final StreamDBClient sdc = new StreamDBClient(config)) {
                // Only the row with epoch_hour referring to 2023-10-5 should be pulled to slicetable.
                int rows = sdc.pullToSliceTable(Date.valueOf("2023-10-5"));
                Assertions.assertEquals(1, rows);
            }
        });
    }

    /**
     * Testing that pullToSliceTable() pulls all the rows from database according to the input parameter.
     */
    @Test
    public void pullToSliceTableMultiTest() {
        // Add test data to logfile table in journaldb.
        final DSLContext ctx = DSL.using(connection, SQLDialect.MYSQL);
        // Set logdate and logtime to 2023-10-04:22 UTC-4 and set epoch_hour in path to 2023-10-05:02 UTC.
        LogfileRecord logfileRecord = logfileRecordForEpoch(1696471200L, false);
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord).execute();
        // Set logdate and logtime to 2023-10-04:23 UTC-4 and set epoch_hour in path to 2023-10-05:03 UTC.
        LogfileRecord logfileRecord2 = logfileRecordForEpoch(1696471200L + 3600L, false);
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord2).execute();

        // Assert StreamDBClient methods work as expected with the test data.
        final Map<String, String> opts = new HashMap<>(this.opts);
        opts.put("DBurl", mariadb.getJdbcUrl());
        final Config config = new Config(opts);
        Assertions.assertDoesNotThrow(() -> {
            try (final StreamDBClient sdc = new StreamDBClient(config)) {
                // Both of the rows in the database with epoch_hour referring to "2023-10-5" should be pulled to the slicetable.
                int rows = sdc.pullToSliceTable(Date.valueOf("2023-10-5"));
                Assertions.assertEquals(2, rows);
            }
        });
    }

    /**
     * Testing that pullToSliceTable() does not pull any rows from the database when the index value in the queryXML
     * does not match with logtag.
     */
    @Test
    public void pullToSliceTableInvalidIndexTest() {
        // Add test data to logfile table in journaldb.
        final DSLContext ctx = DSL.using(connection, SQLDialect.MYSQL);
        // Set logdate and logtime to 2023-10-04:22 UTC-4 and set epoch_hour in path to 2023-10-05:02 UTC.
        LogfileRecord logfileRecord = logfileRecordForEpoch(1696471200L, false);
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord).execute();

        // Assert StreamDBClient methods work as expected with the test data.
        final Map<String, String> opts = this.opts;
        opts.put("DBurl", mariadb.getJdbcUrl());
        opts.put("queryXML", "<index value=\"invalidLogtag\" operation=\"EQUALS\"/>");
        final Config config = new Config(opts);
        Assertions.assertDoesNotThrow(() -> {
            try (final StreamDBClient sdc = new StreamDBClient(config)) {
                // 0 rows should be pulled to sliceTable
                int rows = sdc.pullToSliceTable(Date.valueOf("2023-10-5"));
                Assertions.assertEquals(0, rows);
            }
        });
    }

    /**
     * Testing situation where epoch_hour is used as a source for logtime and logdate fields.
     */
    @Test
    public void epochHourTest() {
        // Add test data to logfile table in journaldb.
        final DSLContext ctx = DSL.using(connection, SQLDialect.MYSQL);
        // Set logdate and logtime to 2023-10-04:22 UTC-4 and set epoch_hour in path to 2023-10-05:02 UTC.
        LogfileRecord logfileRecord = logfileRecordForEpoch(1696471200L, false);
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord).execute();

        // Assert StreamDBClient methods work as expected with the test data.
        final Map<String, String> opts = new HashMap<>(this.opts);
        opts.put("DBurl", mariadb.getJdbcUrl());
        final Config config = new Config(opts);
        final StreamDBClient sdc = Assertions.assertDoesNotThrow(() -> new StreamDBClient(config));
        Long earliestEpoch = 1696377600L; // 2023-10-04
        Long latestOffset = earliestEpoch;

        // Pull the records from a specific logdate to the slicetable for further processing.
        int rows = sdc.pullToSliceTable(Date.valueOf("2023-10-5"));
        Assertions.assertEquals(1, rows);

        // Get the offset for the first non-empty hour of records from the slicetable.
        WeightedOffset nextHourAndSizeFromSliceTable = sdc.getNextHourAndSizeFromSliceTable(latestOffset);
        Assertions.assertFalse(nextHourAndSizeFromSliceTable.isStub);
        latestOffset = nextHourAndSizeFromSliceTable.offset();
        Assertions.assertEquals(1696471200L, latestOffset);
        Result<Record9<ULong, String, String, String, String, String, Long, ULong, ULong>> hourRange = sdc
                .getHourRange(earliestEpoch, latestOffset);
        Assertions.assertEquals(1, hourRange.size());
        // Assert that the resulting logfile metadata is as expected for logtime.
        Assertions.assertEquals(1696471200L, hourRange.get(0).get(6, Long.class));
    }

    /**
     * Testing situation where logfile record hasn't been migrated to use epoch columns. Will use old logdate and
     * synthetic logtime fields instead as a fallback which will trigger the session timezone to affect logtime results.
     */
    @Disabled("Removed support for old logtime source, only epoch_hour supported.")
    @Test
    public void epochHourNullTest() {
        // Add test data to logfile table in journaldb.
        final DSLContext ctx = DSL.using(connection, SQLDialect.MYSQL);
        // Set logdate to 2023-10-04 and set logtime-string in path to 2023100422 UTC-4, but set epoch values to null.
        Instant instant = Instant.ofEpochSecond(1696471200L);
        ZonedDateTime instantZonedDateTime = ZonedDateTime.ofInstant(instant, zoneId);
        LogfileRecord logfileRecord = logfileRecordForEpoch(instantZonedDateTime.toEpochSecond(), true);
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord).execute();

        // Assert StreamDBClient methods work as expected with the test data.
        final Map<String, String> opts = new HashMap<>(this.opts);
        opts.put("DBurl", mariadb.getJdbcUrl());
        final Config config = new Config(opts);
        Assertions.assertDoesNotThrow(() -> {
            try (final StreamDBClient sdc = new StreamDBClient(config)) {
                Instant instantEarliest = Instant.ofEpochSecond(1696392000L);
                ZonedDateTime instantEarliestZonedDateTime = ZonedDateTime.ofInstant(instantEarliest, zoneId);
                long earliestEpoch = instantEarliestZonedDateTime.toEpochSecond(); // 2023-10-04 00:00 UTC-4

                // Pull the records from a specific logdate to the slicetable for further processing.
                int rows = sdc.pullToSliceTable(Date.valueOf(instantEarliestZonedDateTime.toLocalDate()));
                Assertions.assertEquals(1, rows);

                // Get the offset for the first non-empty hour of records from the slicetable.
                WeightedOffset nextHourAndSizeFromSliceTable = sdc.getNextHourAndSizeFromSliceTable(0L);
                Assertions.assertFalse(nextHourAndSizeFromSliceTable.isStub);
                long latestOffset = nextHourAndSizeFromSliceTable.offset();
                // zonedDateTime is used for checking timestamp errors caused by synthetic creation of logtime from logfile path column using regex.
                Assertions.assertEquals(instantZonedDateTime.toEpochSecond(), latestOffset);
                Result<Record9<ULong, String, String, String, String, String, Long, ULong, ULong>> hourRange = sdc
                        .getHourRange(earliestEpoch, latestOffset);
                Assertions.assertEquals(1, hourRange.size());
                // Assert that resulting logfile metadata for logtime is affected by the session timezone when epoch columns are null and session timezone is America/New_York.
                long logtime = hourRange.get(0).get(7, Long.class);
                Assertions.assertEquals(instantZonedDateTime.toEpochSecond(), logtime);
                // Assert that the resulting logfile metadata is as expected for logdate.
                Assertions
                        .assertEquals(Date.valueOf(instantZonedDateTime.toLocalDate()), hourRange.get(0).get(4, Date.class));
            }
        });
    }

    @Test
    public void getNextHourAndSizeFromSliceTableEpochTest() {
        // Add test data to logfile table in journaldb.
        final DSLContext ctx = DSL.using(connection, SQLDialect.MYSQL);
        // Set logdate and logtime to 2023-10-04:18 UTC-4 and set epoch_hour in path to 2023-10-04:22 UTC.
        Instant instant = Instant.ofEpochSecond(1696456800L);
        LogfileRecord logfileRecord = logfileRecordForEpoch(1696456800L, false);
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord).execute();
        // Set logdate and logtime to 2023-10-04:19 UTC-4 and set epoch_hour in path to 2023-10-04:23 UTC.
        LogfileRecord logfileRecord2 = logfileRecordForEpoch(1696456800L + 3600L, false);
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord2).execute();

        // Assert StreamDBClient methods work as expected with the test data.
        final Map<String, String> opts = new HashMap<>(this.opts);
        opts.put("DBurl", mariadb.getJdbcUrl());
        final Config config = new Config(opts);
        final StreamDBClient sdc = Assertions.assertDoesNotThrow(() -> new StreamDBClient(config));
        int rows = sdc.pullToSliceTable(Date.valueOf("2023-10-4"));
        Assertions.assertEquals(2, rows);
        WeightedOffset nextHourAndSizeFromSliceTable = sdc.getNextHourAndSizeFromSliceTable(1696456800L);
        // Assert that the result for next hour from slice table after 2023-10-4 22:00 UTC is 2023-10-4 23:00 UTC.
        Assertions.assertEquals(1696456800L + 3600L, nextHourAndSizeFromSliceTable.offset());
    }

    @Disabled("Removed support for old logtime source, only epoch_hour supported.")
    @Test
    public void getNextHourAndSizeFromSliceTableNullEpochTest() {
        // Add test data to logfile table in journaldb.
        final DSLContext ctx = DSL.using(connection, SQLDialect.MYSQL);
        // Set logdate to 2023-10-04 and set logtime-string in path to 2023100422 UTC-4, but set epoch values to null.
        Instant instant = Instant.ofEpochSecond(1696471200L);
        ZonedDateTime instantZonedDateTime = ZonedDateTime.ofInstant(instant, zoneId);
        ZonedDateTime instantPlusHour = instantZonedDateTime.plusHours(1);
        LogfileRecord logfileRecord = logfileRecordForEpoch(instantZonedDateTime.toEpochSecond(), true);
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord).execute();
        // Set logdate to 2023-10-04 and set logtime-string in path to 2023100423 UTC-4, but set epoch values to null.
        LogfileRecord logfileRecord2 = logfileRecordForEpoch(instantPlusHour.toEpochSecond(), true);
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord2).execute();

        // Assert StreamDBClient methods work as expected with the test data.
        final Map<String, String> opts = new HashMap<>(this.opts);
        opts.put("DBurl", mariadb.getJdbcUrl());
        final Config config = new Config(opts);
        Assertions.assertDoesNotThrow(() -> {
            try (final StreamDBClient sdc = new StreamDBClient(config)) {
                int rows = sdc.pullToSliceTable(Date.valueOf(instantZonedDateTime.toLocalDate()));
                Assertions.assertEquals(2, rows);
                WeightedOffset nextHourAndSizeFromSliceTable = sdc
                        .getNextHourAndSizeFromSliceTable(instantZonedDateTime.toEpochSecond());
                // Assert that the result for next hour from slice table after 2023-10-4 22:00 UTC-4 is 2023-10-4 23:00 UTC-4.
                Assertions.assertEquals(instantPlusHour.toEpochSecond(), nextHourAndSizeFromSliceTable.offset());
            }
        });
    }

    @Test
    public void weightedOffsetTest() {
        // Add test data to logfile table in journaldb.
        final DSLContext ctx = DSL.using(connection, SQLDialect.MYSQL);

        final long baseTime = 1696471200L;

        final long baseMinusOneHour = baseTime - 3600L;
        final long basePlusOneDay = baseTime + (24 * 3600L);

        final LogfileRecord baseRecord = logfileRecordForEpoch(baseTime, false);
        ctx.insertInto(JOURNALDB.LOGFILE).set(baseRecord).execute();
        final LogfileRecord secondRecord = logfileRecordForEpoch(baseTime, false);
        // Set the bucketId to something else to bypass unique key checks.
        secondRecord.setBucketId(UShort.valueOf(2));
        ctx.insertInto(JOURNALDB.LOGFILE).set(secondRecord).execute();
        final LogfileRecord plusOneDayRecord = logfileRecordForEpoch(basePlusOneDay, false);
        ctx.insertInto(JOURNALDB.LOGFILE).set(plusOneDayRecord).execute();

        // Assert StreamDBClient methods work as expected with the test data.
        final Map<String, String> opts = new HashMap<>(this.opts);
        opts.put("DBurl", mariadb.getJdbcUrl());

        final Config config = new Config(opts);
        final StreamDBClient sdc = Assertions.assertDoesNotThrow(() -> new StreamDBClient(config));

        // pull baseTime to SliceTable and assert weightedOffset to contain filesize=240 (2 rows) for that hour
        final int baseTimeRows = sdc.pullToSliceTable(Date.valueOf("2023-10-5"));
        Assertions.assertEquals(2, baseTimeRows);
        final WeightedOffset weightedOffsetForBaseTime = sdc.getNextHourAndSizeFromSliceTable(baseMinusOneHour);

        Assertions.assertEquals(baseTime, weightedOffsetForBaseTime.offset());
        Assertions.assertEquals(240L, weightedOffsetForBaseTime.fileSize());

        // pull baseTime+1day to SliceTable and assert weightedOffset to contain filesize=120 (1 row) for that hour
        final int plusOneDayRows = sdc.pullToSliceTable(Date.valueOf("2023-10-6"));
        Assertions.assertEquals(1, plusOneDayRows);
        final WeightedOffset weightedOffsetForPlusOneDay = sdc.getNextHourAndSizeFromSliceTable(baseTime);
        Assertions.assertEquals(basePlusOneDay, weightedOffsetForPlusOneDay.offset());
        Assertions.assertEquals(120L, weightedOffsetForPlusOneDay.fileSize());
    }

    @Disabled("Removed support for old logtime source, only epoch_hour supported.")
    @Test
    public void weightedOffsetNullEpochTest() {
        // Add test data to logfile table in journaldb.
        final DSLContext ctx = DSL.using(connection, SQLDialect.MYSQL);
        final Instant instant = Instant.ofEpochSecond(1696471200L);

        final ZonedDateTime baseTime = ZonedDateTime.ofInstant(instant, zoneId);
        final ZonedDateTime baseMinusOneHour = baseTime.minusHours(1);
        final ZonedDateTime basePlusOneDay = baseTime.plusDays(1);

        final LogfileRecord baseRecord = logfileRecordForEpoch(baseTime.toEpochSecond(), true);
        ctx.insertInto(JOURNALDB.LOGFILE).set(baseRecord).execute();
        final LogfileRecord secondBaseRecord = logfileRecordForEpoch(baseTime.toEpochSecond(), true);
        // Set the bucketId to something else to bypass unique key checks.
        secondBaseRecord.setBucketId(UShort.valueOf(2));
        ctx.insertInto(JOURNALDB.LOGFILE).set(secondBaseRecord).execute();
        final LogfileRecord plusOneDayRecord = logfileRecordForEpoch(basePlusOneDay.toEpochSecond(), true);
        ctx.insertInto(JOURNALDB.LOGFILE).set(plusOneDayRecord).execute();

        // Assert StreamDBClient methods work as expected with the test data.
        final Map<String, String> opts = new HashMap<>(this.opts);
        opts.put("DBurl", mariadb.getJdbcUrl());

        final Config config = new Config(opts);

        Assertions.assertDoesNotThrow(() -> {
            try (final StreamDBClient sdc = new StreamDBClient(config)) {
                // pull baseTime to SliceTable and assert weightedOffset to contain filesize=240 (2 rows) for that hour
                final int baseTimeRows = sdc.pullToSliceTable(Date.valueOf(baseTime.toLocalDate()));
                Assertions.assertEquals(2, baseTimeRows);
                final WeightedOffset weightedOffsetForBaseTime = sdc
                        .getNextHourAndSizeFromSliceTable(baseMinusOneHour.toEpochSecond());

                Assertions.assertEquals(baseTime.toEpochSecond(), weightedOffsetForBaseTime.offset());
                Assertions.assertEquals(240L, weightedOffsetForBaseTime.fileSize());

                // pull baseTime+1day to SliceTable and assert weightedOffset to contain filesize=120 (1 row) for that hour
                final int plusOneDayRows = sdc.pullToSliceTable(Date.valueOf(basePlusOneDay.toLocalDate()));
                Assertions.assertEquals(1, plusOneDayRows);
                final WeightedOffset weightedOffsetForPlusOneDay = sdc
                        .getNextHourAndSizeFromSliceTable(baseTime.toEpochSecond());
                Assertions.assertEquals(basePlusOneDay.toEpochSecond(), weightedOffsetForPlusOneDay.offset());
                Assertions.assertEquals(120L, weightedOffsetForPlusOneDay.fileSize());
            }
        });
    }

    @Test
    public void deleteRangeRemovesRowsInRange() {
        final DSLContext ctx = DSL.using(connection, SQLDialect.MYSQL);
        // insert two logfiles, one inside range and one outside
        final ZonedDateTime inRangeZdt = ZonedDateTime.of(2023, 10, 4, 22, 0, 0, 0, ZoneId.of("UTC"));
        final LogfileRecord inRangeRecord = logfileRecordForEpoch(inRangeZdt.toEpochSecond(), false);
        ctx.insertInto(JOURNALDB.LOGFILE).set(inRangeRecord).execute();
        final ZonedDateTime outRangeZdt = inRangeZdt.plusDays(1);
        final LogfileRecord outOfRangeRecord = logfileRecordForEpoch(outRangeZdt.toEpochSecond(), false);
        ctx.insertInto(JOURNALDB.LOGFILE).set(outOfRangeRecord).execute();
        final Map<String, String> localOpts = new HashMap<>(opts);
        localOpts.put("DBurl", mariadb.getJdbcUrl());
        final Config config = new Config(localOpts);
        final StreamDBClient sdc = Assertions.assertDoesNotThrow(() -> new StreamDBClient(config));
        final int pulledInRange = sdc.pullToSliceTable(Date.valueOf(inRangeZdt.toLocalDate()));
        final int pulledOutOfRange = sdc.pullToSliceTable(Date.valueOf(outRangeZdt.toLocalDate()));
        Assertions.assertEquals(1, pulledInRange);
        Assertions.assertEquals(1, pulledOutOfRange);
        final int deletedRows = sdc
                .deleteRangeFromSliceTable(inRangeZdt.minusHours(1).toEpochSecond(), inRangeZdt.toEpochSecond());
        Assertions.assertEquals(1, deletedRows, "in range logfile should be deleted from slice table");
        final WeightedOffset remaining = sdc.getNextHourAndSizeFromSliceTable(0L);
        Assertions.assertFalse(remaining.isStub, "out of range logfile should remain in slice table");
    }

    @Disabled("Removed support for old logtime source, only epoch_hour supported.")
    @Test
    public void deleteRangeRemovesNullLogtimeRowsOutsideOfRange() {
        final DSLContext ctx = DSL.using(connection, SQLDialect.MYSQL);
        final ZonedDateTime recordZdt = ZonedDateTime.of(2020, 1, 2, 3, 4, 0, 0, zoneId);
        final long recordEpochSeconds = recordZdt.toEpochSecond();
        final LogfileRecord logfileRecord = new LogfileRecord(
                ULong.valueOf(recordEpochSeconds),
                Date.valueOf(recordZdt.toLocalDate()),
                Date.valueOf(recordZdt.plusYears(1).toLocalDate()),
                UShort.valueOf(1),
                "invalid/path",
                null,
                UShort.valueOf(1),
                "filename",
                new Timestamp(recordEpochSeconds),
                ULong.valueOf(120L),
                "sha256 checksum 1",
                "archive tag 1",
                "oldExample",
                UShort.valueOf(2),
                UShort.valueOf(1),
                ULong.valueOf(390L),
                null,
                null,
                null,
                ULong.valueOf(1),
                null
        );
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord).execute();
        final Map<String, String> localOpts = new HashMap<>(opts);
        localOpts.put("DBurl", mariadb.getJdbcUrl());
        final Config config = new Config(localOpts);
        final StreamDBClient sdc = Assertions.assertDoesNotThrow(() -> new StreamDBClient(config));
        int pulled = sdc.pullToSliceTable(Date.valueOf(recordZdt.toLocalDate()));
        final ZonedDateTime baseTime = ZonedDateTime.of(2023, 10, 4, 22, 0, 0, 0, ZoneId.of("UTC"));
        Assertions.assertEquals(1, pulled, "row should be pulled to slice table");
        final int deleted = sdc
                .deleteRangeFromSliceTable(baseTime.minusHours(1).toEpochSecond(), baseTime.toEpochSecond());
        Assertions.assertEquals(1, deleted, "row should be deleted even when it is outside of range");
        Assertions
                .assertTrue(sdc.getNextHourAndSizeFromSliceTable(0L).isStub, "slice table should be empty after delete");
    }

    /**
     * Testing deleteRangeFromSliceTable() method functionality with new epoch logtime implementation.
     */
    @Test
    public void deleteRangeFromSliceTableTest() {
        // Add test data to logfile table in journaldb.
        final DSLContext ctx = DSL.using(connection, SQLDialect.MYSQL);
        // Inserting logfile with logtime of 2023-10-05 02:00 UTC.
        Instant instant = Instant.ofEpochSecond(1696471200L);
        LogfileRecord logfileRecord = logfileRecordForEpoch(instant.getEpochSecond(), false);
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord).execute();

        // Assert StreamDBClient methods work as expected with the test data.
        final Map<String, String> opts = new HashMap<>(this.opts);
        opts.put("DBurl", mariadb.getJdbcUrl());
        final Config config = new Config(opts);
        final StreamDBClient sdc = Assertions.assertDoesNotThrow(() -> new StreamDBClient(config));

        // Pull the records from a specific logdate to the slicetable for further processing.
        int rows = sdc.pullToSliceTable(Date.valueOf("2023-10-5"));
        Assertions.assertEquals(1, rows);
        Assertions.assertFalse(sdc.getNextHourAndSizeFromSliceTable(0L).isStub);

        // Delete rows from slicetable and assert that they are no longer present in the slicetable.
        sdc.deleteRangeFromSliceTable(instant.minusSeconds(3600).getEpochSecond(), instant.getEpochSecond());
        Assertions.assertTrue(sdc.getNextHourAndSizeFromSliceTable(0L).isStub);
    }

    /**
     * Testing deleteRangeFromSliceTable() method functionality with old logtime implementation.
     */
    @Disabled("Removed support for old logtime source, only epoch_hour supported.")
    @Test
    public void deleteRangeFromSliceTableNullEpochTest() {
        // Add test data to logfile table in journaldb.
        final DSLContext ctx = DSL.using(connection, SQLDialect.MYSQL);
        // Inserting logfile with logtime of 2023-10-04 22:00 UTC-4.
        Instant instant = Instant.ofEpochSecond(1696471200L);
        ZonedDateTime instantZonedDateTime = ZonedDateTime.ofInstant(instant, zoneId);
        LogfileRecord logfileRecord = logfileRecordForEpoch(instantZonedDateTime.toEpochSecond(), true);
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord).execute();

        // Assert StreamDBClient methods work as expected with the test data.
        final Map<String, String> opts = new HashMap<>(this.opts);
        opts.put("DBurl", mariadb.getJdbcUrl());
        final Config config = new Config(opts);
        Assertions.assertDoesNotThrow(() -> {
            try (final StreamDBClient sdc = new StreamDBClient(config)) {
                // Pull the records from a specific logdate to the slicetable for further processing.
                int rows = sdc.pullToSliceTable(Date.valueOf(instantZonedDateTime.toLocalDate()));
                Assertions.assertEquals(1, rows);
                Assertions.assertFalse(sdc.getNextHourAndSizeFromSliceTable(0L).isStub);

                // Delete rows from slicetable and assert that they are no longer present in the slicetable.
                sdc
                        .deleteRangeFromSliceTable(instantZonedDateTime.minusHours(1).toEpochSecond(), instantZonedDateTime.toEpochSecond());
                Assertions.assertTrue(sdc.getNextHourAndSizeFromSliceTable(0L).isStub);
            }
        });
    }

    /**
     * Testing IncludeBeforeEpoch functionality with new epoch logtime implementation.
     */
    @Test
    public void setIncludeBeforeEpochTest() {

        // Add test data to logfile table in journaldb.
        final DSLContext ctx = DSL.using(connection, SQLDialect.MYSQL);
        // Set logdate and logtime to 2023-10-04:22 UTC-4 and set epoch_hour in path to 2023-10-05:02 UTC.
        Instant instant = Instant.ofEpochSecond(1696471200);
        Instant instantPlusHour = instant.plusSeconds(3600);
        LogfileRecord logfileRecord = logfileRecordForEpoch(instant.getEpochSecond(), false);
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord).execute();
        // Inserting logfile with logtime of 2023-10-05 03:00 UTC.
        LogfileRecord logfileRecord2 = logfileRecordForEpoch(instantPlusHour.getEpochSecond(), false);
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord2).execute();

        // Assert StreamDBClient methods work as expected with the test data.

        // Set includeBeforeEpoch in ArchiveConfig to an epoch that represents 2023-10-05 03:00 UTC, for getNextHourAndSizeFromSliceTable() to ignore records with logtime of 2023-10-05 03:00 UTC or newer.
        final Map<String, String> opts = new HashMap<>(this.opts);
        opts.put("DBurl", mariadb.getJdbcUrl());
        opts.put("archive.includeBeforeEpoch", String.valueOf(instantPlusHour.getEpochSecond()));
        final Config config = new Config(opts);
        final StreamDBClient sdc = Assertions.assertDoesNotThrow(() -> new StreamDBClient(config));

        // Pull the records from a specific logdate to the slicetable for further processing.
        int rows = sdc.pullToSliceTable(Date.valueOf("2023-10-5"));
        Assertions.assertEquals(2, rows);

        // find the earliest row and assert that it has correct offset/logtime value
        Assertions.assertFalse(sdc.getNextHourAndSizeFromSliceTable(0L).isStub);
        Assertions.assertEquals(instant.getEpochSecond(), sdc.getNextHourAndSizeFromSliceTable(0L).offset());
        // find the next row after earliest and assert that it is stub.
        Assertions.assertTrue(sdc.getNextHourAndSizeFromSliceTable(instant.getEpochSecond()).isStub);

    }

    /**
     * Testing IncludeBeforeEpoch functionality with old logtime implementation.
     */
    @Disabled("Removed support for old logtime source, only epoch_hour supported.")
    @Test
    public void setIncludeBeforeEpochNullEpochTest() {

        // Add test data to logfile table in journaldb.
        final DSLContext ctx = DSL.using(connection, SQLDialect.MYSQL);
        // Set logdate and logtime to 2023-10-04:22 UTC-4 and set epoch_hour in path to 2023-10-05:02 UTC.
        Instant instant = Instant.ofEpochSecond(1696471200L);
        ZonedDateTime instantZonedDateTime = ZonedDateTime.ofInstant(instant, zoneId);
        ZonedDateTime instantPlusHour = instantZonedDateTime.plusHours(1);
        LogfileRecord logfileRecord = logfileRecordForEpoch(instantZonedDateTime.toEpochSecond(), true);
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord).execute();
        // Inserting logfile with logtime of 2023-10-04 23:00 UTC-4.
        LogfileRecord logfileRecord2 = logfileRecordForEpoch(instantPlusHour.toEpochSecond(), true);
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord2).execute();

        // Assert StreamDBClient methods work as expected with the test data.

        // Set includeBeforeEpoch in ArchiveConfig to an epoch that represents 2023-10-04 23:00 UTC-4, for getNextHourAndSizeFromSliceTable() to ignore records with logtime of 2023-10-04 23:00 UTC-4 or newer.
        final Map<String, String> opts = new HashMap<>(this.opts);
        opts.put("DBurl", mariadb.getJdbcUrl());
        opts.put("archive.includeBeforeEpoch", String.valueOf(instantPlusHour.toEpochSecond()));
        final Config config = new Config(opts);
        Assertions.assertDoesNotThrow(() -> {
            try (final StreamDBClient sdc = new StreamDBClient(config)) {
                // Pull the records from a specific logdate to the slicetable for further processing.
                int rows = sdc.pullToSliceTable(Date.valueOf(instantZonedDateTime.toLocalDate()));
                Assertions.assertEquals(2, rows);

                // find the earliest row and assert that it has correct offset/logtime value
                Assertions.assertFalse(sdc.getNextHourAndSizeFromSliceTable(0L).isStub);
                Assertions
                        .assertEquals(instantZonedDateTime.toEpochSecond(), sdc.getNextHourAndSizeFromSliceTable(0L).offset());
                // find the next row after earliest and assert that it is stub.
                Assertions
                        .assertTrue(sdc.getNextHourAndSizeFromSliceTable(instantZonedDateTime.toEpochSecond()).isStub);
            }
        });
    }

    /**
     * Testing that normalized logtag is handled properly in the queries.
     */
    @Test
    public void logtagIdTest() {
        // Add test data to logfile table in journaldb.
        final DSLContext ctx = DSL.using(connection, SQLDialect.MYSQL);
        // Set logdate and logtime to 2023-10-04:22 UTC-4 and set epoch_hour in path to 2023-10-05:02 UTC.
        Instant instant = Instant.ofEpochSecond(1696471200L);
        LogfileRecord logfileRecord = logfileRecordForEpoch(instant.getEpochSecond(), false);
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord).execute();

        // Assert StreamDBClient methods work as expected with the test data.
        final Map<String, String> opts = this.opts;
        opts.put("DBurl", mariadb.getJdbcUrl());
        // Set queryXML to search for logfiles with logtag of example, which is used for new normalized logtag in the inserted logfiles.
        opts.put("queryXML", "<index value=\"example\" operation=\"EQUALS\"/>");
        final Config config = new Config(opts);
        Assertions.assertDoesNotThrow(() -> {
            try (final StreamDBClient sdc = new StreamDBClient(config)) {
                Instant instantEarliest = Instant.ofEpochSecond(1696392000L);
                ZonedDateTime instantEarliestZonedDateTime = ZonedDateTime.ofInstant(instantEarliest, zoneId);
                final long earliestEpoch = instantEarliestZonedDateTime.toEpochSecond(); // 2023-10-04 00:00 UTC-4

                // Pull the records from a specific logdate to the slicetable for further processing.
                int rows = sdc.pullToSliceTable(Date.valueOf("2023-10-5"));
                Assertions.assertEquals(1, rows);

                // Get the offset for the first non-empty hour of records from the slicetable.
                WeightedOffset nextHourAndSizeFromSliceTable = sdc.getNextHourAndSizeFromSliceTable(0L);
                Assertions.assertFalse(nextHourAndSizeFromSliceTable.isStub);
                final long latestOffset = nextHourAndSizeFromSliceTable.offset();
                // Get the record from slicetable and assert that it was found with the queryXML condition.
                Result<Record9<ULong, String, String, String, String, String, Long, ULong, ULong>> hourRange = sdc
                        .getHourRange(earliestEpoch, latestOffset);
                Assertions.assertEquals(1, hourRange.size());
            }
        });
    }

    /**
     * Testing that the old logtag column is not used in the queries.
     */
    @Test
    public void logtagTest() {
        // Add test data to logfile table in journaldb.
        final DSLContext ctx = DSL.using(connection, SQLDialect.MYSQL);
        // Set logdate and logtime to 2023-10-04:22 UTC-4 and set epoch_hour in path to 2023-10-05:02 UTC.
        Instant instant = Instant.ofEpochSecond(1696471200L);
        LogfileRecord logfileRecord = logfileRecordForEpoch(instant.getEpochSecond(), false);
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord).execute();

        // Assert StreamDBClient methods work as expected with the test data.
        final Map<String, String> opts = this.opts;
        opts.put("DBurl", mariadb.getJdbcUrl());
        // Set queryXML to search for logfiles with logtag of oldExample, which is used for old logtag column in the inserted logfiles.
        opts.put("queryXML", "<index value=\"oldExample\" operation=\"EQUALS\"/>");
        final Config config = new Config(opts);
        Assertions.assertDoesNotThrow(() -> {
            try (final StreamDBClient sdc = new StreamDBClient(config)) {
                // Pull the records from a specific logdate to the slicetable for further processing.
                int rows = sdc.pullToSliceTable(Date.valueOf("2023-10-5"));
                // Assert that no rows were pulled to slicetable because of queryXML condition.
                Assertions.assertEquals(0, rows);
            }
        });
    }

    @Test
    public void pullToSliceTableSkipsCorruptedLogfilesTest() {
        // Add test data to logfile table in journaldb.
        final DSLContext ctx = DSL.using(connection, SQLDialect.MYSQL);
        // Set logdate and logtime to 2023-10-04:22 UTC-4 and set epoch_hour in path to 2023-10-05:02 UTC.
        Instant instant = Instant.ofEpochSecond(1696471200L);
        LogfileRecord corruptedLogfileRecord = logfileRecordForEpoch(instant.getEpochSecond(), false);
        ctx.insertInto(JOURNALDB.LOGFILE).set(corruptedLogfileRecord).execute();
        // Add the ID of the inserted logfile to corrupted_archive table
        CorruptedArchiveRecord corruptedArchiveRecord = new CorruptedArchiveRecord(corruptedLogfileRecord.getId());
        int insertedRows = ctx.insertInto(JOURNALDB.CORRUPTED_ARCHIVE).set(corruptedArchiveRecord).execute();
        Assertions.assertEquals(1, insertedRows);
        final Map<String, String> opts = this.opts;
        opts.put("DBurl", mariadb.getJdbcUrl());
        final Config config = new Config(opts);
        final StreamDBClient sdc = Assertions.assertDoesNotThrow(() -> new StreamDBClient(config));
        // Pull the records from a specific logdate to the slicetable for further processing.
        int rows = sdc.pullToSliceTable(Date.valueOf("2023-10-5"));
        // Assert that the record with ID present in corrupted_archive table is not included in the query result
        Assertions.assertEquals(0, rows);
        WeightedOffset nextHourAndSizeFromSliceTable = sdc.getNextHourAndSizeFromSliceTable(0L);
        Assertions.assertTrue(nextHourAndSizeFromSliceTable.isStub);
    }

    /**
     * Testing timezone handling of epoch_hour and logtime near midnight using 2 different session timezones.
     */
    @Test
    public void epochHourTimezoneTest() {
        // Add test data to logfile table in journaldb.
        final DSLContext ctx = DSL.using(connection, SQLDialect.MYSQL);
        // Set logdate and logtime to 2023-10-04:21 UTC-4 and set epoch_hour in path to 2023-10-05:01 UTC.
        LogfileRecord logfileRecord = logfileRecordForEpoch(1696467600L, false);
        // Insert the logfileRecord to the database using JOOQ.
        ctx.insertInto(JOURNALDB.LOGFILE).set(logfileRecord).execute();

        // Create an instance of StreamDBClient using the default server timezone (UTC-4).
        final Map<String, String> opts = new HashMap<>(this.opts);
        opts.put("DBurl", mariadb.getJdbcUrl());
        final Config config = new Config(opts);
        final StreamDBClient sdc = Assertions.assertDoesNotThrow(() -> new StreamDBClient(config));
        // Create another instance of StreamDBClient using explicitly set UTC session timezone.
        final Map<String, String> optsUTC = this.opts;
        optsUTC.put("DBurl", mariadb.getJdbcUrl() + "?forceConnectionTimeZoneToSession=true&connectionTimeZone=UTC");
        final Config configUTC = new Config(optsUTC);
        final StreamDBClient sdcUTC = Assertions.assertDoesNotThrow(() -> new StreamDBClient(configUTC));

        final Long earliestEpoch = 1696377600L; // 2023-10-04
        Long latestOffset = earliestEpoch;

        // Pull the records from a specific logdate to the slicetable for further processing.
        int rows = sdc.pullToSliceTable(Date.valueOf("2023-10-5"));
        Assertions.assertEquals(1, rows);
        // Do the same for sdcUTC
        Assertions.assertEquals(rows, sdcUTC.pullToSliceTable(Date.valueOf("2023-10-5")));

        // Get the offset for the first non-empty hour of records from the slicetable.
        WeightedOffset nextHourAndSizeFromSliceTable = sdc.getNextHourAndSizeFromSliceTable(latestOffset);
        Assertions.assertFalse(nextHourAndSizeFromSliceTable.isStub);
        // Do the same for sdcUTC
        WeightedOffset nextHourAndSizeFromSliceTableUTC = sdcUTC.getNextHourAndSizeFromSliceTable(latestOffset);
        Assertions.assertFalse(nextHourAndSizeFromSliceTableUTC.isStub);

        latestOffset = nextHourAndSizeFromSliceTable.offset();
        Assertions.assertEquals(latestOffset, nextHourAndSizeFromSliceTableUTC.offset());

        // Get the logfile results from the known hour range.
        Assertions.assertEquals(1696467600L, latestOffset);
        Result<Record9<ULong, String, String, String, String, String, Long, ULong, ULong>> hourRange = sdc
                .getHourRange(earliestEpoch, latestOffset);
        Assertions.assertEquals(1, hourRange.size());
        // Do the same for sdcUTC
        Result<Record9<ULong, String, String, String, String, String, Long, ULong, ULong>> hourRangeUTC = sdcUTC
                .getHourRange(earliestEpoch, latestOffset);
        Assertions.assertEquals(1, hourRangeUTC.size());
        // Assert that the resulting logfile metadata is as expected for logtime, they should not be affected by session timezone.
        ZonedDateTime zonedDateTimeUTC = ZonedDateTime.of(2023, 10, 5, 1, 0, 0, 0, ZoneId.of("UTC"));
        Assertions.assertEquals(zonedDateTimeUTC.toEpochSecond(), hourRange.get(0).get(6, Long.class));
        Assertions.assertEquals(zonedDateTimeUTC.toEpochSecond(), hourRangeUTC.get(0).get(6, Long.class));
    }

    @Test
    public void equalsHashCodeContractTest() {
        EqualsVerifier
                .forClass(StreamDBClient.class)
                .withIgnoredFields("nestedTopNQuery")
                .withPrefabValues(NestedTopNQuery.class, new NestedTopNQuery(null, false), new NestedTopNQuery(null, false)).verify();
    }
}
